{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nreal_data = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/True.csv')\nfake_data = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/Fake.csv') \n\nreal_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fake_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will then add a column to each of the dataset telling if the row is fake new or not"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"real_data['is_fake'] = False\nfake_data['is_fake'] = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"real_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fake_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I will put both of the dataset into the same dataframe to use it more easily"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import shuffle\n\ndata = pd.concat([real_data, fake_data])\n\n# Shuffle the data\ndata = shuffle(data).reset_index(drop=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Now for the fun part\nI will split the data into three sets: training, validation and testing sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data, validate_data, test_data = np.split(data.sample(frac=1), [int(.6*len(data)), int(.8*len(data))])\n\ntrain_data = train_data.reset_index(drop=True)\nvalidate_data = validate_data.reset_index(drop=True)\ntest_data = test_data.reset_index(drop=True)\n\nprint(\"Size of training set: {}\".format(len(train_data)))\nprint(\"Size of validation set: {}\".format(len(validate_data)))\nprint(\"Size of testing set: {}\".format(len(test_data)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Installing the required libraries:"},{"metadata":{"trusted":true},"cell_type":"code","source":"!conda install -y pytorch torchvision cudatoolkit=10.1 -c pytorch\n!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\ndevice = torch.device('cuda')\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\").to(device)\n\ncriterion = nn.MSELoss().to(device)\noptimizer = optim.SGD(model.parameters(), lr=0.001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_every = 300\n\ntotal_loss = 0\nall_losses = []\n\nfor idx, row in train_data.iterrows():\n    text = str(row['text'])\n    splitted_text = text.split(' ')\n    try:\n        label = torch.tensor([row['is_fake']]).float().to(device)\n    except:\n        print(torch.tensor([row['is_fake']]).float())\n\n    # text = tokenizer.encode(text, return_tensors=\"pt\")\n    # Split the text in parts of 500 characters and run the classification on each part\n\n    parts = []\n\n    text_len = len(text.split(' '))\n    delta = 250\n    max_parts = 5\n    nb_cuts = int(text_len / delta)\n\n    for i in range(nb_cuts + 1):\n        text_part = ' '.join(splitted_text[i * delta: (i + 1) * delta])\n        parts.append(tokenizer.encode(text_part[:max_parts], return_tensors=\"pt\").to(device))\n\n    optimizer.zero_grad()\n\n    overall_output = torch.zeros((1,2)).to(device)\n    try:\n        for part in parts:\n            if len(part) > 0:\n                overall_output += model(part.reshape(1, -1))[0]\n    except RuntimeError:\n        print(\"GPU out of memory, skipping this entry.\")\n        continue\n\n    overall_output /= len(parts)\n\n    if label == 0:\n        label = torch.tensor([[1.0, 0.0]]).to(device)\n    elif label == 1:\n        label = torch.tensor([[0.0, 1.0]]).to(device)\n\n    loss = criterion(overall_output, label)\n    total_loss += loss\n    loss.backward()\n    optimizer.step()\n\n    if idx % print_every == 0 and idx > 0:\n        average_loss = total_loss / print_every\n        print(\"{}/{}. Average loss: {}\".format(idx, len(train_data), average_loss))\n        all_losses.append(average_loss)\n        total_loss = 0\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n%matplotlib inline\ntorch.save(model.state_dict(), \"model_after_train.pt\")\n\nplt.plot(all_losses)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now I will test the accuracy of the model on the test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}